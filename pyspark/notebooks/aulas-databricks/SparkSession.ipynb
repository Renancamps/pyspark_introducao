{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a100a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f3ff2-bec3-4e6a-89c2-b73fc6416010",
   "metadata": {},
   "source": [
    "### **Spark Session** ###\n",
    "A Spark Session é o ponto de entrada pra acessar todas as funcionalidades do Spark. Por meio dela, é possível: <br>\n",
    "• **Ler e criar DataFrames;<br>\n",
    "• Realizar queries do SQL;<br>\n",
    "• Configurar a aplicação;<br>\n",
    "• Acessar o catálogo de metadados.**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e05e6-ef88-45d0-a273-b23b9952ffd0",
   "metadata": {},
   "source": [
    "### **Metodos mais comuns** ###\n",
    "\n",
    "- version() – Returns the Spark version where your application is running, probably the Spark version your cluster is configured with. <br>\n",
    "\n",
    "- createDataFrame() – Cria um Dataframe de uma coleção  <br>\n",
    "\n",
    "- getActiveSession() – Retorna a SparkSession ativa ativa <br>\n",
    "\n",
    "- read() – Usado para ler registros de csv, parquet, avro, delta entre outros formatos num DataFrame. <br>\n",
    "\n",
    "- readStream() – Usado para ler dados streaming num DataFrame. <br>\n",
    "\n",
    "- sparkContext() – Retorna um SparkContext. <br>\n",
    "\n",
    "- sql() – Retorna um Dataframe equivalente a execução de uma instrução SQL. <br>\n",
    "\n",
    "- sqlContext() – Retorna um SQLContext. <br>\n",
    "\n",
    "- stop() – Para o SparkContext atual. <br>\n",
    "\n",
    "- table() – Retorna um Dataframe de uma tabela ou view. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be44e7-fb5d-477d-ab9c-e0f5eaa5f273",
   "metadata": {},
   "source": [
    "### **Criando uma Spark Session** ###\n",
    "Algumas formas de criar uma sparkSession: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdb486e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/13 11:44:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158c8eb-3b30-4be4-b9d0-1e994cfc1ad8",
   "metadata": {},
   "source": [
    "Adicionando configurações a sparkSession durante a criação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a88482d-4775-40d0-b2b0-94355723fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    ".config('spark.executor.memory', '8G')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0960728-2d75-4c51-9cad-63de1021721b",
   "metadata": {},
   "source": [
    "Verificando o valor de alguma configuração no spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe3b604-2b15-4b9e-9c04-fc1cb217cb2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785d3bf-fe6d-4dcf-b9f6-694ea4c2353b",
   "metadata": {},
   "source": [
    "### **Criando uma Spark Context** ###\n",
    "Desde o spark 2.0 , a nova forma de acessar driver do spark é com uma SparkSession, atravez da SparkSession temos  <br>\n",
    "Acesso ao SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12bade9a-40f3-4cca-8317-b94800ee6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkContext=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb175ab-06aa-40aa-80f8-04416e6ed57a",
   "metadata": {},
   "source": [
    "### **Criando um RDD** ###\n",
    "Utilizando o metodo parallelize, podemos criar um RDD , passando uma lista como fonte de dados, por exemplo.<br>\n",
    "Com o RDD criado, podemos realizar uma série de ações e transformações . Abaixo executaremos algumas demonstrações de utilização.\n",
    "### **Metodos mais comuns de um RDD** ###\n",
    "\n",
    "- count() – Retorna a quantidade de elementos do RDD <br>\n",
    "\n",
    "- collect() – Retorna uma lista com os elementos do RDD  <br>\n",
    "\n",
    "- foreach(f) – Executa uma função para cada elemento do RDD, recebe como parametro uma função <br>\n",
    "\n",
    "- filter(f) – Retorna um novo RDD que satisfaça a função dentro do filter. <br>\n",
    "\n",
    "- map(f) – Retorna um novo RDD aplicando a função para cada elemento. <br>\n",
    "\n",
    "- reduce() – ?????????. <br>\n",
    "\n",
    "- join() – Retorna um novo RDD com os elementos que correspondam a uma chave . <br>\n",
    "Esse metodo é utilizado para executar uma junção entre dois RDD's <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51166980-6e95-4738-a71e-b2d887b79569",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"scale\",\"Java\",\"Hadoop\",\"spark\",\"Akka\",\"spark vs Hadoop\",\"pyspark\",\"pyspark and spark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b946dd2b-971c-4b24-984d-7f380c73f6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd = sparkContext.parallelize(words)\n",
    "type(words_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6023a96a-4a27-4404-8079-8fc7329def29",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = words_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f360ffeb-558f-4a03-bdb6-a82e41be2bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c8b378a-8165-4fd7-8eab-4abc5ff0e8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scale',\n",
       " 'Java',\n",
       " 'Hadoop',\n",
       " 'spark',\n",
       " 'Akka',\n",
       " 'spark vs Hadoop',\n",
       " 'pyspark',\n",
       " 'pyspark and spark']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cab38c02-eb37-4cb8-a720-33e7d8c11444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e10c129-86aa-4bcc-97a1-6e01eba76cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spark\n",
      "pyspark\n",
      "Hadoop\n",
      "spark vs hadoop\n",
      "scale\n",
      "Java\n",
      "pyspark and spark\n",
      "Akka\n"
     ]
    }
   ],
   "source": [
    "def f(x): print(x)\n",
    "    \n",
    "fore = words.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a0140db-24c6-45c9-9605-fdb74ff3939d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = word_filter.collect()\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47c856e4-c5a4-4baf-bbeb-ddeb6aff555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark vs hadoop', 'pyspark and spark']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_filter = words.filter(lambda x: len(x) > 7)\n",
    "filtered = word_filter.collect()\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "158ec9c2-36d9-4248-8685-b5bbfd025514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scale', 1),\n",
       " ('Java', 1),\n",
       " ('Hadoop', 1),\n",
       " ('spark', 1),\n",
       " ('Akka', 1),\n",
       " ('spark vs hadoop', 1),\n",
       " ('pyspark', 1),\n",
       " ('pyspark and spark', 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_map = words.map(lambda x: (x, 1))\n",
    "mapping = word_map.collect()\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3162224-99bf-4ffd-bdd5-7ab593ce18e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'operator' from 'pyspark' (/usr/local/spark/python/pyspark/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m operator\n\u001b[1;32m      2\u001b[0m nums \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m      3\u001b[0m add \u001b[38;5;241m=\u001b[39m nums\u001b[38;5;241m.\u001b[39mreduce(add)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'operator' from 'pyspark' (/usr/local/spark/python/pyspark/__init__.py)"
     ]
    }
   ],
   "source": [
    "????? exemplo de reduce\n",
    "nums = sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "add = nums.reduce(add)\n",
    "(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79d88cfd-c0b4-420b-a6bd-4a67385190fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark', (1, 2))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sparkContext.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "rdd2 = sparkContext.parallelize([(\"spark\", 2), (\"Hadoop\", 5)])\n",
    "joined = rdd1.join(rdd2)\n",
    "result = joined.collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16ec2a3c-8b42-4e7e-8bd8-c395cfa2ae5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3ad74-ef37-4ca6-9b5d-2df239afae38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb73db0-1904-4f6d-a8ab-725878210a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98192611",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "301bbc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0b580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "007e78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c8adc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69766ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b34a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet(\"../data/people/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format('parquet').load('../data/people/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1619cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.format(\"delta\").save(\"../data/delta/people/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00850fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b17d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
